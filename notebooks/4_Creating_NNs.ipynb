{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41857f03-9449-48e5-b6ba-7433cae0da13",
   "metadata": {},
   "source": [
    "![banner](../static/banner.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3dc624-4f61-4538-8687-641212089efa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 4 - Creating neural networks to classify handwritten digits üî¢‚úçüèæ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfdffd-cf94-4c0e-b850-1691ffbf6621",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classifying handwritten digits 2Ô∏è‚É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e285bbb-d7f2-4b3b-92ad-906dc40bacc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Consider the following sequence of handwritten digits.\n",
    "\n",
    "![handwritten-digits](../static/digit_seq.png)\n",
    "\n",
    "Many of us learn to how to read such numerals at a young age, and can effortlessly recognize those digits as 504192. But how could we go about getting a computer to recognise these digits?\n",
    "\n",
    "We could try programming in some of our intuitions and heuristics for recognising digits, such as \"a 2 has an arch at the top, connected to a horizontal stroke at the bottom\". But some twos are written with a loop at the bottom, some are written a bit thicker, some have blotches, and some don't even really look like twos!\n",
    "\n",
    "![twos](../static/twos.png)\n",
    "\n",
    "While the human brain seems to easily identify all these symbols as twos, it turns out explicitly defining rules for identifying a two is incredibly difficult. But if we can't figure out exactly how to tell the computer to recognise a two, maybe we can try and help it learn itself? It turns out this works surprisingly well! Even the most basic neural network, which we'll create below, can do this fairly well. \n",
    "\n",
    "But before we dive into creating a neural network, we need a source of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154af825-66ee-4b16-bcad-38549699c701",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The MNIST database üßÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9666c-783a-4954-b65f-c659752a4421",
   "metadata": {},
   "source": [
    "The [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) is a large (and now, after being used in many machine learning tutorials, famous) database containing thousands of handwritten digits, from 0 through 9, written by samples of American high-school students and employees from the American Census Bureau. If we were dealing with data in the real world, it would be all kinds of messy and require susbtantial preprocesing. Luckily, the [creators of this database](http://yann.lecun.com/exdb/mnist/) have done all the work for us in making the data nice and clean.\n",
    "\n",
    "![MNIST](../static/rand_digits.png)\n",
    "\n",
    "Each image of a handwritten digit has been reduced to a square 28 by 28 pixel image, for a total of **784 pixels**. Each digit is centered in the image. Because these images are greyscale, we can describe the appearance of each pixel in one number, representing the \"brightness\". We'll say that a completely white pixel has the value 0, and a completely black pixel the value of 1. Using this encoding, we can represent each handwritten digit image by 28 by 28 matrix. An example of how the image of the handwritten 1 on the left would be encoded as a matrix is shown below.\n",
    "\n",
    "![MNIST](../static/mnist.png)\n",
    "\n",
    "This matrix can then be \"unfolded\" into a single list of 784 values between 0 and 1. This is how the data is formatted and stored in the `.csv` file we'll used to train and test our neural network. Let's load in the data now and take a look.\n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> Run the cell below to load the data. What do the rows and columns represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92503399-8b0e-4c45-9628-e616e0b5b0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the csv\n",
    "library(data.table)\n",
    "mnist = data.table::fread(\"../data/MNIST_normalised.csv\")\n",
    "                    \n",
    "# Convert the data into a matrix to make it easier to do math\n",
    "mnist = as.matrix(mnist)\n",
    "\n",
    "# Name rows and columns for clarity\n",
    "rownames(mnist) = c(\"label\", paste0(\"pixel\", 1:(nrow(mnist) - 1)))\n",
    "colnames(mnist) = 1:ncol(mnist)\n",
    "\n",
    "# Print the number of rows and columns in the data set\n",
    "cat(paste0(\"Rows: \", nrow(mnist), \"\\nColumns: \", ncol(mnist)))\n",
    "\n",
    "# Display a subset of the data set\n",
    "mnist[c(1:4, 124:127, 783:785), 1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35859da1-5eb4-4cbf-a05d-1f7c2c428a45",
   "metadata": {},
   "source": [
    "Each of the 42,000 columns in the data corresponds to a distinct handwritten digit example. The first row tells us what digit is represented in that column, and the remaining 784 rows encode the brightness of the 784 pixels in each image, on a scale of 0 to 1. (Note that, unlike most datasets, each observation is given a column, not a row. This is to help make coding the maths easier.)\n",
    "\n",
    "We're almost done getting our data ready, the last step is to split our data. We do this in two ways:\n",
    "1. Into training and test sets. We'll take the first 4000 columns as our testing data-set, and leave the rest to help train our network. \n",
    "2. Into the pixel values (`X_train` and `X_test`) and the labels (`Y_train` and `Y_test`).\n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> Why do we split the data the second way? What would happen if we didn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d2649-edd7-4173-96a7-a24a83266c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nrow(mnist)\n",
    "m = ncol(mnist)\n",
    "\n",
    "test = mnist[, 1:4000]\n",
    "Y_test = test[1, ]\n",
    "X_test = test[2:n, ]\n",
    "\n",
    "train = mnist[, 4001:m]\n",
    "Y_train = train[1, ]\n",
    "X_train = train[2:n, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d4509-f1d3-4d61-b7df-72eb6f0bdcea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Planning our neural network ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd944cca-48fa-4577-b9ba-58770bdebc49",
   "metadata": {},
   "source": [
    "So we're going to try and create a neural network to help us classify these digits, but what type of neural network? There are many types of neural networks out there (come along to Part 3 of series to learn more on this), but for today, lets keep things relatively simple. We'll create a multilayer perceptron (MLP) with one hidden layer.\n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> In the empty cell below, write down what key components and/or processes you think we might need to build in order to create our neural network. This can be a list of specific functions, or a general description of things it will need to be able to do. When you're ready, open the collapsed cell below to read how we'll go about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942bc1e-4347-4849-a911-0250fbc73744",
   "metadata": {},
   "source": [
    "`Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda9c6a-9058-41d5-b004-0c4e775712e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Using our code, we'll need to be able to:\n",
    "- Initialise the weights and biases of our network\n",
    "- Apply an activation function\n",
    "- Pass data forward through the network to make predictions (forward propagation)\n",
    "- Comparing predictions against actual values, figure out how we should adjust the weights and biases (back propagation)\n",
    "- Update the weights and biases\n",
    "- Use forward propagation and backpropagation iteratively to train our model (gradient descent)\n",
    "- Measure how good the predictions are\n",
    "- Test the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc79ba5-5332-471a-94e5-4269782dec13",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's code it up üë©‚Äçüíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a40e2-843f-4794-89d3-f3d05e01b3ac",
   "metadata": {},
   "source": [
    "While we already sorted out loading in our training and test data, there's a few more things we need to do before we can get started:\n",
    "1. Load in the `ggplot2` library to help plot some graphs.\n",
    "2. Set the random number generator seed, so that our results are reproducible.\n",
    "3. Define a few **helper functions** to simplify some of the neural network code. \n",
    "\n",
    "We've defined a few helper functions to make a few boring things (like viewing progress while training our network) easier. **You don't need to understand these, and we recommend you don't spend much looking at them**. The names should help indicate what they do,  but feel free to take a look if you're curious!\n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> Run the two cells below (watch out, one of them is collapsed to save space) to complete the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906c372-f027-4c2e-9191-6856a7e77d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601feccf-167c-4af4-be97-771caa74f657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot = function(Y) {\n",
    "  # Converts Y from a list of numbers (categorical variables) to a matrix.\n",
    "  # In this matrix, each column corresponds to a digit. In each column, all\n",
    "  # entries are zero except there is a single 1. The row that this 1 is in\n",
    "  # encodes what number the digit encoded in that column is.\n",
    "  # For example, a 1 in column 78, row 6 of the matrix would mean that the 78th \n",
    "  # digit in Y is a 5 (since row numbers start from 1, but our lowest digit is 0).\n",
    "  one_hot_Y = matrix(0, length(Y), max(Y) + 1)\n",
    "  for(i in 1:length(Y)) {\n",
    "    one_hot_Y[i, Y[i] + 1] = 1\n",
    "  }\n",
    "  one_hot_Y = t(one_hot_Y)\n",
    "  return(one_hot_Y)\n",
    "}\n",
    "\n",
    "print_training_progress = function(nn, Y, iteration) {\n",
    "    # Prints to console some information as the network is being trained\n",
    "    cat(\"-*-\")\n",
    "    flush.console()\n",
    "\n",
    "    if (iteration %% 10 == 0) {\n",
    "      cat(\"\\n\")\n",
    "      predictions = get_predictions(nn)\n",
    "      accuracy = get_accuracy(predictions, Y)\n",
    "      cat(paste0(\"Iteration: \", iteration, \", Accuracy: \", round(accuracy, 2), \"\\n\"))\n",
    "      flush.console()\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_activation = function(activation_function, x = seq(-7,7,0.001)) {\n",
    "    # Used to plot the activation function\n",
    "    df = data.frame(x = x, y = activation_function(x))\n",
    "    options(repr.plot.width = 12, repr.plot.height = 5)\n",
    "    \n",
    "    ggplot(df, aes(x, y)) +\n",
    "      geom_line(lwd = 1.5) +\n",
    "      theme_minimal() +\n",
    "      theme(text = element_text(size = 20), element_line(size = 1)) +\n",
    "      ylab(paste0(\"y = \", deparse(substitute(activation_function)), \"(x)\"))\n",
    "}\n",
    "\n",
    "display_digit = function(X, digit_index, label = TRUE) {\n",
    "    # Plots a visual representation of a digit.\n",
    "    digit = matrix(X[, digit_index], nrow = 28, ncol = 28)\n",
    "    digit = digit[, nrow(digit):1]     # The image function is annoying, it flips matrices upside down, this fixes it.\n",
    "    image(1:28, 1:28, digit, col = gray((255:0)/255.), axes = FALSE, xlab = \"\", ylab = \"\")\n",
    "    box(col = \"black\") \n",
    "    if(label) {\n",
    "        title(paste0(\"Digit label: \", as.character(Y_train[digit_index])), col.main = \"black\", line = 0.7, cex.main = 2)\n",
    "    }\n",
    "}\n",
    "\n",
    "display_incorrect = function(incorrect, predictions, X_test, Y_test) {\n",
    "    # Plots the first 24 incorrect predictions by the nn.\n",
    "    options(repr.plot.width = 10, repr.plot.height = 10)\n",
    "    par(\n",
    "        mfcol = c(4, 6), \n",
    "        mar = c(0, 0, 8, 0)\n",
    "    )\n",
    "    for (id in incorrect[1:24]) {\n",
    "        digit = matrix(X_test[, id], nrow = 28, ncol = 28)\n",
    "        digit = digit[, nrow(digit):1]\n",
    "        image(1:28, 1:28, digit, col = gray((255:0)/255), axes = FALSE)\n",
    "        title(\n",
    "            paste0(\"Actual: \", as.character(Y_test[id]), \"\\n Predicted: \", predictions[id]), \n",
    "            col.main = \"black\", \n",
    "            line = 0.7, \n",
    "            cex.main = 2\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0328e6-0139-4442-99f2-77fcbba9d3e9",
   "metadata": {},
   "source": [
    "### A word of advice üí°\n",
    "As we jump into the code below, don't stress if things don't click straight away (especially if you don't have much experience). Where you can, see if you can read through the code, line by line, to get a good idea of what exactly is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbbfe8-cd6e-496f-b74a-395b3b1a6124",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The activation function\n",
    "Here we define the activation function we'll use in our network (and it's derivative). The function we'll use for our network is a classic, called the ***sigmoid function***, given by:\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7741fb7-22b4-4494-8a88-7d5b7821fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = function(Z) {\n",
    "  # The sigmoid function.\n",
    "  return(1/(1 + exp(-Z)))\n",
    "}\n",
    "\n",
    "sigmoid_derivative = function(Z) {\n",
    "  # The derivative of the sigmoid function.\n",
    "  return(sigmoid(Z) * (1 - sigmoid(Z)))\n",
    "}\n",
    "\n",
    "plot_activation(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff7624-7db0-4eb5-91be-30917b87a272",
   "metadata": {},
   "source": [
    "### Initialising the neural network\n",
    "In order to train our network to recognise digits, we first have to create it! \n",
    "\n",
    "The function below initialises our neural network with random weights and biases (sampled from a standard normal), and returns them as a list. As described above, we focus on creating a simple network with one hidden layer.\n",
    "\n",
    "Recall that if there are $n$ nodes in one layer and $m$ nodes in the next layer, then we'll have $n \\times m$ weights in total. We neatly store these in a matrix, where each row contains all the weights linking to a given neuron in the next layer.\n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> How many neurons should we have in our input layer? How about in our output layer? Replace the `???` in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2657feb-7519-47a8-abea-5eb9d106d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_net = function(num_hidden_nodes = 16) {\n",
    "  # Initialises a neural network with one hidden layer.\n",
    "  # Takes in an integer representing the number of nodes to be placed in the hidden layer.\n",
    "  # Returns a list containing randomly initialised weights and biases.\n",
    "    \n",
    "  num_layers = 3 # One input layer, one hidden layer and one output layer for a total of three layers\n",
    "  n_h = num_hidden_nodes # Change name for brevity\n",
    "  \n",
    "  n_input = ??? # Number of neurons in the input layer\n",
    "  n_output = ??? # Number of neurons in the output layer\n",
    "\n",
    "  nn = list(\n",
    "    num_layers = num_layers,\n",
    "    num_hidden_nodes = n_h,\n",
    "    # Create a matrix to hold all the weights from the input layer to the hidden layer\n",
    "    W1 = matrix(data = rnorm(n_input * num_hidden_nodes), nrow = n_h, ncol = n_input),\n",
    "    b1 = rnorm(n_h),\n",
    "    # Create a matrix to hold all the weights from the hidden layer to the output layer\n",
    "    W2 = matrix(data = rnorm(n_h * n_output), nrow = n_output, ncol = n_h),\n",
    "    b2 = rnorm(n_output)\n",
    "  ) \n",
    "  return(nn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e1030-0a22-4a94-b600-31cef266df4f",
   "metadata": {},
   "source": [
    "Run the cell below to initialise a neural network and view its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59602a03-0112-4816-8bdd-b3a6b0e843ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352dfc65-f478-429c-9d73-416fe6b0c30c",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "The process of passing information into the input layer and propagating it through the network is called forward propagation (or forward passing). The function below uses matrix arithmetic to pass through all our data at once. This function looks a little scary, and uses some maths, but all its really doing is pushing all all the data in matrix `X` through the network in one neat expression.\n",
    "\n",
    "Note that here `%*%` is the operator we use for matrix multiplication.\n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> There are **two things missing** from the forward propagation code below, identify what's missing and fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4508213-9806-479f-bff5-d46f0d42c610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forward_prop = function(nn, X) {\n",
    "  nn$Z1 = (nn$W1 %*% X)\n",
    "  nn$A1 = sigmoid(nn$Z1)\n",
    "  nn$Z2 = (nn$W2 %*% nn$A1) + nn$b2\n",
    "  nn$A2 = nn$Z2\n",
    "  return(nn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87072db-a27e-4cac-9004-66a68a8e305e",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "In order to train our network, we measure how well our network is performing, and adjust the weights and biases accordingly. This is where ***backpropagation*** comes in. \n",
    "\n",
    "It calculuates the difference between the predicted values and actual values in the output layer, and pushes these differences through the network from back to front. Using some multivariable calculus (the chain rule) and linear algebra, allows us to figure out how best to change our weights and biases to improve the accuracy of the network.\n",
    "\n",
    "If you haven't done much linear algebra and calculus before (and even if you have) the math can be a bit tricky, so don't worry too much about exactly what's going on here. For now, the overarching idea that backprop helps inform us about how we should change our weights and biases is more important than the precise mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffccc5-bc57-4d47-9c17-b101bf08f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_prop = function(nn, X, Y) {\n",
    "  dZ2 = nn$A2 - one_hot(Y)\n",
    "  nn$dW2 = 1 / m * dZ2 %*% t(nn$A1)\n",
    "  nn$db2 = 1 / m * sum(dZ2)\n",
    "  dZ1 = t(nn$W2) %*% dZ2 * sigmoid_derivative(nn$Z1)\n",
    "  nn$dW1 = 1 / m * dZ1 %*% t(X)\n",
    "  nn$db1 = 1 / m * sum(dZ1)\n",
    "  return(nn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3774f-6ebd-447e-9296-91d2342ad755",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Updating the network's parameters\n",
    "Running backpropagation tells us whether we should increase or decrease our each of our weights and biases, and comparatively, how \"important\" changes are relative to eachother, but it doesn't actually change them, nor does it tell us by how much we should actually change them!\n",
    "\n",
    "The update_params function allows us to use the information we got from running the backpropogation function to actually change our weights and biases. The rate at which these are changed is determined by what we call the ***learning rate***. Learning rate is an example of what is called a ***hyper-parameter***, a value that controls the \"learning\" process, and isn't set by training. \n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> Fill in the function body so that it updates all the parameters of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53379ef5-0f0c-448a-ae4d-1c88125f272d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "*Hint:* Look at the values that the `back_prop` function adds to our network. What do they mean? \n",
    "*Another hint:* You can do this in four lines of code, all of which will be pretty similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621025b-8c0a-4b12-8d97-3d4761193350",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_params = function(nn, learning_rate) {\n",
    "    \n",
    "    # Write your code here!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c42121-974d-4546-a69c-971b92e5d090",
   "metadata": {},
   "source": [
    "### Training the network - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87bf4a-840d-4e4b-afb6-d8a5e57c74eb",
   "metadata": {},
   "source": [
    "We've now got most of the building blocks in place! Before we train our network, we need to be able to get predictions and calculate accuracy. We get predictions by taking whichever neuron in the output layer returns the highest value, and we define accuracy as the percentage of inputs that our neural network correctly classifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df86916-b332-4d0c-9c63-3d087edb32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions = function(nn) {\n",
    "  # - 1 to account for the fact that R indexes from 1 not 0\n",
    "  predictions = apply(nn$A2, 2, which.max) - 1\n",
    "  return(predictions)\n",
    "}\n",
    "\n",
    "get_accuracy = function(predictions, Y) {\n",
    "  return(sum(predictions == Y)/length(Y))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e3ffc-ce31-4682-b64f-e0c7824e2eb0",
   "metadata": {},
   "source": [
    "The process of iteratively doing forward propagation, backpropagation and updating the weights and biases of the neural network is known as **gradient descent**. \n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> Complete the for loop below to define the gradient descent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54482127-c93f-49a6-8096-0605fbb3e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent = function(nn, X, Y, learning_rate, iterations) {\n",
    "  for (i in 1:iterations) {\n",
    "      \n",
    "    # Write your code here\n",
    "      \n",
    "    print_training_progress(nn, Y, i)\n",
    "  }\n",
    "  return(nn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e3cc1-14f3-4351-ba10-a5c040511005",
   "metadata": {},
   "source": [
    "If everything has gone right above, you should be able to run the cell below to train the network! \n",
    "\n",
    "<font color='#F89536'> **Your turn:** </font> Run the cell below to train the network. If there's no errors, kick back and relax for a few minutes while it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed94a1-a186-41c8-8949-01a179726491",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = initialise_net()\n",
    "nn = gradient_descent(nn, X_train, Y_train, learning_rate = 1, iterations = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1368b-01dd-48dd-9ce8-219d5af439b7",
   "metadata": {},
   "source": [
    "### Testing the network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e4351-eacb-43a5-a8fe-0981b391693c",
   "metadata": {},
   "source": [
    "But how good is our network? Let's test it on some data it hasn't seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d12e4-bce1-4ba8-87e9-7b166dc4f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = forward_prop(nn, X_test)\n",
    "predictions = get_predictions(nn)\n",
    "accuracy = get_accuracy(predictions, Y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed68f88-3a25-433d-bb1a-9514a7f6ba52",
   "metadata": {},
   "source": [
    "Roughly 80% accuracy. Not bad! This is a lot better than guessing randomly. Let's take a look at some of the digits that the network *incorrectly classified*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f60902-7cc7-4794-9ba8-7b5ebdcb0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = which(predictions != Y_test)\n",
    "display_incorrect(incorrect, predictions, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f67d6a-8e3b-45a7-88e6-1bd79e81ac1d",
   "metadata": {},
   "source": [
    "Some of these mistakes are pretty easy to understand, some of these digits are a bit whacky! But for others, it's pretty clear what the digits are, and its a bit puzzling as to why the network couldn't get them.\n",
    "\n",
    "There's obviously lots of room for improvement here; we've barely scratched the surface! Using more complicated neural networks, its possible to get an accuracy very close to 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da406da0-024b-449b-8a82-788cf2e0f2bc",
   "metadata": {},
   "source": [
    "## That's it! ü•≥\n",
    "Congratulations, if you've made it this far you understand the basics of how to implement a simple neural network from scratch! Before you go, here's a few final things to consider:\n",
    "- Today we created a neural network from scratch. In practice, no-one really does this (why re-invent the wheel?) Instead, we use specialised machine learning packages and libraries to help make this process easier.\n",
    "- While the handwritten digit problem is a nice introduction to neural networks, its definitely on the simple side. It's simple enough that we can actually get decent results with linear models, and other ML models like SVMs and random forests. For more complicated problems, such as recognising faces, our simple NN wouldn't fare so well. Tune into the next session to learn more about neural networks that are better at that kind of thing!\n",
    "- If you're keen to dive deeper into this example, try some of the extra challenges below.\n",
    "- If you want to learn more about coding in R, machine learning or neural networks, check out the resources suggested in Part 5, linked below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f72201-6138-4fef-9b77-bd424377d632",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extra challenges üî• "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f19e6a-f115-47e9-8aa8-88d48a4725ca",
   "metadata": {},
   "source": [
    "If you're keen to dive a little bit deeper into this example, here are some more challenging tasks to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0d2cd-8053-466e-9041-2715cd4d2a37",
   "metadata": {},
   "source": [
    "### Mild üå∂Ô∏è üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d54de-cb5e-476f-bbe3-3c31a7e4255c",
   "metadata": {},
   "source": [
    "Suitable for beginners, no experience expected.\n",
    "\n",
    "- **Setting the learning rate:** We didn't talk much about the learning rate. Try training the network with a few different values. What happens?\n",
    "\n",
    "- **Increasing the accuracy of the model:** \n",
    "The current ~80% accuracy is not bad, but leaves room for improvement. Play around with the parameters of the model and the training processs to see if you can get a better accuracy.\n",
    "\n",
    "- **Leaving in the labels:** When we created our testing data, we stripped out the labels. What would happen if we kept the labels, and removed some other row instead? Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968a99d-dbf2-4281-98ed-c0eee7959472",
   "metadata": {},
   "source": [
    "### Spicy üå∂Ô∏èüå∂Ô∏è üò≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe477fa5-8368-41aa-bd37-886317d94059",
   "metadata": {},
   "source": [
    "Coding experience and/or some math experience will be helpful, you might need to write a little bit of code yourself. \n",
    "\n",
    "- **Understanding the math of forward prop:** Take a look back at the forward propagation function, and try to develop an understanding of exactly what's going on.\n",
    "\n",
    "- **Passing in random noise:** Create a matrix full of random values and feed it through a trained network. What happens? Does this make sense?\n",
    "\n",
    "- **Visualising weights:** Visualise the weights that connect all neurons in the input layer to the first neuron in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153134f9-bc25-4010-8238-3ce666ccf558",
   "metadata": {},
   "source": [
    "### Flaming üå∂Ô∏èüå∂Ô∏èüå∂Ô∏è ü•µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf20b2-3a1c-4c8c-a45b-d1da22bfe9e5",
   "metadata": {},
   "source": [
    "Coding & statistical experience, patience and high level of enthusiasm required, these aren't easy! \n",
    "\n",
    "- **Understanding the math of backward prop:** Take a look back at the backward propagation function, and try to develop an understanding of exactly what's going on.\n",
    "\n",
    "- **Trying other statistical models:** Although we've looked at using neural networks to classify digits, this problem is relatively simple, and other statistical methods can get high accuracies too. Try another model (e.g an SVM or a random forest) and see how the performance compares. \n",
    "\n",
    "- **More hidden layers:** Extend the model so that an arbitrary number of hidden layers can be used.\n",
    "\n",
    "- **Using a NN package:** Above, we coded up our neural network from scratch to improve our understanding. In practice, people pre-written code to create models. Have a look at the [neuralnet](https://cran.r-project.org/web/packages/neuralnet/index.html) package, and try to train it using the MNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbdc42-6939-4da2-af83-9e79d6b08935",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## ‚Ü™Ô∏è Next up: [Part 5 üöÄ - Discovering more](./5_Discovering_More.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
